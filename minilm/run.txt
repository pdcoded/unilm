Unlit training struggle

kill -15 -1

kill -9 -1

Apt-get update
apt-get install zip

Unzip *.zip



alias=`whoami | cut -d'.' -f2`; docker run -it --runtime=nvidia --ipc=host --privileged -v /home/${alias}:/home/${alias} unilm:latest bash



docker run -it  --runtime=nvidia --ipc=host --privileged pytorch/pytorch:1.2-cuda10.0-cudnn7-devel bash 


https://github.com/pdcoded/unilm.git
Pip install -r requirements.txt

git clone https://github.com/NVIDIA/apex.git && cd apex && pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" .


TRAIN_FILE=qg_data/train.qg.uc.json
OUTPUT_DIR=output_dir
CACHE_DIR=cache_dir


python -m torch.distributed.launch --nproc_per_node=4 ./s2s-ft/run_seq2seq.py \
  --train_file ${TRAIN_FILE} --output_dir ${OUTPUT_DIR} \
  --model_type unilm --model_name_or_path unilm1.2-base-uncased \
  --do_lower_case --fp16 --fp16_opt_level O2 --max_source_seq_length 128 --max_target_seq_length 64 \
  --per_gpu_train_batch_size 16 --gradient_accumulation_steps 1 \
  --learning_rate 7e-5 --num_warmup_steps 50 --num_training_steps 1000 --cache_dir ${CACHE_DIR}


python -m torch.distributed.launch --nproc_per_node=1 ./s2s-ft/run_seq2seq.py \
  --train_file ${TRAIN_FILE} --output_dir ${OUTPUT_DIR} \
  --model_type minilm --model_name_or_path minilm-l12-h384-uncased \
  --do_lower_case --fp16 --fp16_opt_level O2 --max_source_seq_length 64 --max_target_seq_length 48 \
  --per_gpu_train_batch_size 16 --gradient_accumulation_steps 1 \
  --learning_rate 7e-5 --num_warmup_steps 50 --num_training_steps 1000 --num_training_epochs 5 --save_steps 1000 --cache_dir ${CACHE_DIR}





# path of the fine-tuned checkpoint
MODEL_PATH=minilm-l12-h384-uncased
MODEL_RECOVER_PATH=output_dir/model.1000.bin
SPLIT=dev
INPUT_JSON=qg_data/test.qg.uc.json
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

python ./s2s-ft/decode_seq2seq.py \
  --fp16 --model_type minilm --tokenizer_name minilm-l12-h384-uncased --input_file ${INPUT_JSON} --split $SPLIT --do_lower_case \
 --model_recover_path ${MODEL_RECOVER_PATH} --max_seq_length 512 --max_tgt_length 48 --batch_size 32 --beam_size 5 \
 --length_penalty 1.2 --forbid_duplicate_ngrams --mode s2s --forbid_ignore_word "." --min_len 6

Remove shuffle argument


docker commit 0723111ac51d unilm:version2

docker cp 507f46b89515:/workspace/unilm/output_dir .

cached_features_for_training.pt  model.100.bin  optim.100.bin  train_opt.json








TRAIN_FILE=qg_data/train.qg.uc.json
OUTPUT_DIR=output_dir
CACHE_DIR=cache_dir


python -m torch.distributed.launch --nproc_per_node=4 ./s2s-ft/run_seq2seq.py \
  --train_file ${TRAIN_FILE} --output_dir ${OUTPUT_DIR} \
  --model_type unilm --model_name_or_path unilm1.2-base-uncased \
  --do_lower_case --fp16 --fp16_opt_level O2 --max_source_seq_length 464 --max_target_seq_length 48 \
  --per_gpu_train_batch_size 16 --gradient_accumulation_steps 1 \
  --learning_rate 7e-5 --num_warmup_steps 500 --num_training_steps 48000 --cache_dir ${CACHE_DIR}



MODEL_NAME=minilm-l12-h384-uncased
MODEL_RECOVER_PATH=output_dir/ckpt-1000
SPLIT=dev
INPUT_JSON=qg_data/test.qg.uc.json
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

python ./s2s-ft/decode_seq2seq.py \
  --fp16 --model_type minilm --tokenizer_name minilm-l12-h384-uncased --input_file ${INPUT_JSON} --split $SPLIT --do_lower_case \
 --model_path ${MODEL_RECOVER_PATH} --max_seq_length 512 --max_tgt_length 48 --batch_size 32 --beam_size 5 \
 --length_penalty 1.2 --forbid_duplicate_ngrams --mode s2s --forbid_ignore_word "." --min_len 6 